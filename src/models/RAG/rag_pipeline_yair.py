"""
This script implements the pipeline for Q&A using RAG with an entire conversation as its context, trying to find the topics in a conversation as
answers to questions about the text. It uses a sliding window.
"""

from langchain.chains.llm import LLMChain
from langchain_core.prompts import PromptTemplate

import get_model
from preprocessing_for_RAG import preprocess_for_RAG  # Performs preprocessing that concatenates all utterances for RAG model
from prompts_yair import topic_prompts, prompt_templates
from src.utils.constants import raw_data_df

# Choosing and loading the model
llm = get_model.mistral()

# Load raw dataframe and filter for a single conversation
df = raw_data_df.copy()
df_single_conv = df[df['recording_id'] == 'd5c13967-e360-4be9-b8d4-1485a1830c89']  # Replace as needed

# Preprocess data to concatenate all utterances for each recording_id into a single text
entire_convs_dict = preprocess_for_RAG(df_single_conv)

# Choose the version of the prompt template and questions
selected_prompt_version = 'v1'  # Change this to the selected version to test
selected_question_version = 'v1'  # Change this to the selected version to test

# Define the prompt template and questions
questions = topic_prompts(version=selected_question_version)
questions_str = "\n".join([q[list(q.keys())[0]] for q in questions])  # Get the question text from each dictionary
prompt_template = prompt_templates(version='v1')

# Create the prompt
prompt = PromptTemplate(
   input_variables=["context", "questions"],
   template=prompt_template,
)

# Create Language Model Chain
llm_chain = LLMChain(llm=llm, prompt=prompt)


def sliding_window_context(full_text, window_size, step_size) -> str:
    """
    Generate sliding windows of text from the full text.

    :param full_text: The complete text to split into windows.
    :param window_size: The number of words in each window.
    :param step_size: The number of words to move the window by.
    :return: The next window of text.
    """
    words = full_text.split()
    for start in range(0, len(words), step_size):
        end = min(start + window_size, len(words))
        yield " ".join(words[start:end])
        if end == len(words):
            break


def answer_questions_with_sliding_window(full_text: str, questions: list, llm_chain, window_size=512, step_size=256) -> list:
    """
    Answer questions using a sliding window approach on the full text.

    :param full_text: The full text of the conversation.
    :param questions: The list of questions to answer.
    :param llm_chain: The language model chain to use for generating answers.
    :param window_size: The size of each window in words. Defaults to 512.
    :param step_size: The step size to move the window by. Defaults to 256.
    :return: The answers generated by the model.
    """
    all_answers = []
    i=1
    for window in sliding_window_context(full_text, window_size, step_size):
        print(f'window {i}:')
        print(window)
        print()
        input_data = {"context": window, "questions": "\n".join([q[list(q.keys())[0]] for q in questions])}
        window_answers = llm_chain.invoke(input_data)
        all_answers.append(window_answers)
        i+=1
    return all_answers


# Process each recording_id
for recording_id, full_text in entire_convs_dict.items():
    answers = answer_questions_with_sliding_window(full_text, questions, llm_chain)
    print(f"Answers for recording_id {recording_id}:")
    for i, question in enumerate(questions):
        question_text = question[list(question.keys())[0]]
        print(f"Q: {question_text}")
        for window_answer in answers:
            print(f"A: {window_answer[i]}")
        print('---')

pass
